{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-b921ef4f48ec>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-b921ef4f48ec>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <small><i>This notebook was create by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<small><i>This notebook was create by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part II - Classification (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based and proximal algorithms on the binary classification problems with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification and Logistic Regression\n",
    "\n",
    "### Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$ .\n",
    "\n",
    "\n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "$$ \\min_{x\\in\\mathbb{R}^d} \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$\n",
    "\n",
    "\n",
    "### Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ).\n",
    "\\end{align*}\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "We will use LibSVM formatted data, meaning that each line of the file (i.e. each example) will have the form\n",
    "\n",
    "<tt>class feature_number1:feature_value1 feature_number2:feature_value2 ... feature_number$n_i$:feature_value$n_i$ </tt>\n",
    "\n",
    "You may read such a file using MLUtils's <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile\">`loadLibSVMFile`</a> routine on the supervised classification datasets below.\n",
    "\n",
    "The elements of the produced RDD have the form of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoints`</a> composed of a label `example.label` corresponding to the class (+1 or -1) and a feature vector `example.features` generally encoded as a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector\">`SparseVector`</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up spark environment (using Spark local mode set to # cores on your machine)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part II - Logistic Regression\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface (Spark UI) by simply opening http://localhost:4040 in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to LibSVM Datasets\n",
    "LibSVMHomeDir=\"data/LibSVM/\"\n",
    "LibName=\"ionosphere.txt\"             # a small dataset to begin with\n",
    "#LibName=\"rcv1_train.binary\"          # a bigger one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    "> Form an RDD from the selected dataset.\n",
    "\n",
    "> Count the number of examples, features, the number of examples of class '+1' and the density of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  351\n",
      "The number of features is :  34\n",
      "The number of examples of class +1 is :  225\n",
      "Density : 0.884112619406737\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import KernelDensity\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import os.path\n",
    "\n",
    "path = os.path.join(LibSVMHomeDir,LibName)\n",
    "data = MLUtils.loadLibSVMFile(sc,path)\n",
    "\n",
    "def nbExamples(rdd) :\n",
    "    return rdd.count()\n",
    "def nbFeatures(rdd) :\n",
    "    nbFeatures = rdd.map(lambda line : line.features.size).sum()\n",
    "    return int(nbFeatures/rdd.count())\n",
    "def density(rdd) :\n",
    "    nbFeatures = rdd.map(lambda line : line.features.size).sum()\n",
    "    nonZeros = data.map(lambda line: line.features.numNonzeros()).sum()\n",
    "    return nonZeros/nbFeatures\n",
    "\n",
    "print(\"Number of examples: \",nbExamples(data))\n",
    "print(\"The number of features is : \",nbFeatures(data))\n",
    "nbClassPlus1Examples = data.map(lambda line: line.label).filter(lambda s : float(s) == 1)\n",
    "print(\"The number of examples of class +1 is : \",nbExamples(nbClassPlus1Examples))\n",
    "print(\"Density :\",density(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "An important first step for learning by regression is to preprocess the dataset. This processing usually consists in:\n",
    "* Adding an intercept, that is an additional feature equal to one for all examples (statistically, this accounts for the fact that the two classes may be imbalanced).\n",
    "* For the dense datasets:\n",
    "    *  normalize to have zero-mean and unit variance for every feature (except the interecept for instance.\n",
    "* For sparse datasets:\n",
    "    * normalize so that the feature vector has unit $\\ell_2$ norm for each example.\n",
    "\n",
    "This does not really change the problem but it will ease the convergence of the applied optimization algorithms.\n",
    "\n",
    "__Question 2__\n",
    "> Form a new RDD with the scaled version of the dataset.\n",
    "\n",
    "> Check that the number of examples, features, and the density is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the dataset : 351 \n",
      "Number of examples in normalized data : 351\n",
      "\n",
      "Number of features in the dataset : 34 \n",
      "Number of features in normalized data : 35\n",
      "\n",
      "Density of the dataset : 0.884112619406737 \n",
      "Density of the normalized dataset : 0.8588522588522588\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Normalizer\n",
    "from pyspark.mllib.util import *\n",
    "\n",
    "normalizedData = data.map(lambda line : LabeledPoint(line.label,Normalizer().transform(line.features.toArray())))\n",
    "scaledNormData = normalizedData.map(lambda line : LabeledPoint(line.label,np.append(line.features.toArray(),1)))\n",
    "\n",
    "print(\"Number of examples in the dataset :\",nbExamples(data),\"\\n\\\n",
    "Number of examples in normalized data :\",nbExamples(scaledNormData))\n",
    "\n",
    "print(\"\\nNumber of features in the dataset :\",nbFeatures(data),\"\\n\\\n",
    "Number of features in normalized data :\",nbFeatures(scaledNormData))\n",
    "\n",
    "print(\"\\nDensity of the dataset :\",density(data),\"\\n\\\n",
    "Density of the normalized dataset :\",density(scaledNormData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialization\n",
    "\n",
    "We will set up here the variables, and the training versus testing dataset. Indeed, we will take a portion of the dataset to learn called the `learning set`, say $95$%, and we will test our predictions on the rest, the `testing set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "\n",
    ">  Split the scaled dataset into a training and a testing set. For instance, you may use the function <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\">`randomSplit`</a>.\n",
    "\n",
    "> Count the number of examples, and subjects in class '+1' in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples in the dataset :  351\n",
      "The number of examples in the training dataset :  340\n",
      "The number of examples in the testing dataset :  11\n",
      "\n",
      "The number of examples in class +1 dataset :  225\n",
      "The number of examples in class +1 training dataset :  217\n",
      "The number of examples in class +1 testing dataset:  8\n"
     ]
    }
   ],
   "source": [
    "trainingData, testingData = scaledNormData.randomSplit([95, 5])\n",
    "print(\"The number of examples in the dataset : \",nbExamples(scaledNormData))\n",
    "print(\"The number of examples in the training dataset : \",nbExamples(trainingData))\n",
    "print(\"The number of examples in the testing dataset : \",nbExamples(testingData))\n",
    "\n",
    "classPlus1Data = scaledNormData.filter(lambda line : line.label == 1)\n",
    "classPlus1TrainingData = trainingData.filter(lambda line : line.label == 1)\n",
    "classPlus1TestingData = testingData.filter(lambda line : line.label == 1)\n",
    "print(\"\\nThe number of examples in class +1 dataset : \",nbExamples(classPlus1Data))\n",
    "print(\"The number of examples in class +1 training dataset : \",nbExamples(classPlus1TrainingData))\n",
    "print(\"The number of examples in class +1 testing dataset: \",nbExamples(classPlus1TestingData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Minimization of the logistic loss with the Gradient algorithm\n",
    "\n",
    "The goal of this section is to: \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement a Gradient algorithm.\n",
    "3. Observe the prediction accuracy of the developed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4__\n",
    ">Define a routine computing functional loss and gradient from one example \n",
    "\n",
    "For a Labeled point <tt>example</tt> (`LabeledPoint(example.label,example.features)`) that we denoted $(b_i,a_i)$ and a regressor <tt>x</tt>, compute $f_i(x) = \\log(1+\\exp(-b_i \\langle a_i,x\\rangle) )$ and $\\nabla f_i(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic loss for the first example is :\n",
      " 0.4326745940907781\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "from random import *\n",
    "\n",
    "import numpy as np\n",
    "def logistic_loss_per_example(example,x):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        real value: l \n",
    "    \"\"\"\n",
    "    f = log(1 + exp(-1 * example.label * np.dot(example.features,x)))\n",
    "\n",
    "    return f\n",
    "\n",
    "def randomArray(size) :\n",
    "    x = np.zeros(size)\n",
    "    for i in range(1,size) :\n",
    "        x[i] = uniform(-1,1)\n",
    "    return x\n",
    "\n",
    "size = trainingData.first().features.size\n",
    "testLoss = trainingData.map(lambda line : logistic_loss_per_example(line,randomArray(size)))\n",
    "print(\"The logistic loss for the first example is :\\n\",testLoss.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic grad for the first example is :\n",
      " [-0.109386701112,-0.0,-0.108882428419,0.00644178282846,-0.0932445056285,-0.00252245732763,-0.091226320993,0.0412475372551,-0.109386701112,-0.00411293996179,-0.0932445056285,0.0194216087824,-0.0653640232492,0.0491638528146,-0.0662183333849,0.0418108787659,-0.0922742455897,0.0421598223424,-0.0636761864511,0.0352137668218,-0.0623186974903,0.0324594096878,-0.0404140105927,0.0518022600454,-0.0621436787685,0.0559742688258,-0.0449338690826,0.0505016521692,-0.0232621758584,0.0372899264089,-0.0462344769588,0.0596015318347,-0.0203907749542,0.0495521756035,-0.351228428437]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def logistic_grad_per_example(example,x):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\n",
    "    Args:\n",
    "        example: a labeled point\n",
    "        x: regressor\n",
    "    Returns:\n",
    "        numpy array: g \n",
    "    \"\"\"\n",
    "    g = (-1 * example.label * example.features)/(1 + exp(example.label * np.dot(example.features,x)))\n",
    "    return  g\n",
    "\n",
    "testGradientLoss = trainingData.map(lambda line : logistic_grad_per_example(line,randomArray(size)))\n",
    "print(\"The logistic grad for the first example is :\\n\",testGradientLoss.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5__\n",
    ">Implement a gradient descent algorithm to minimize\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) = \\frac{1}{m}  \\sum_{i=1}^m f_i(x).\n",
    "\\end{align*}\n",
    ">by \n",
    "* defining a function taking a stepsize and a maximal number of iterations and returning the final point as well as the value of $f(x)$ at each iteration. \n",
    "* running `x, f_tab = grad_algo(gamma,MAX_ITE)`\n",
    "\n",
    "\n",
    "For the choice of the stepsize, we help you by provinding you an upper bound on the Lipschitz constant $L$ of $\\nabla f$:\n",
    "\n",
    "$ L \\leq L_b = \\max_i 0.25 \\|a_i\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limite : 1.9999999999999991\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def compute_gamma1():\n",
    "    Lb = 0.25 * trainingData.map(lambda line : pow(np.linalg.norm(line.features,2),2)).max()\n",
    "    return 1/Lb\n",
    "print(\"Limite :\",compute_gamma1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad_algo(gamma, MAX_ITER):\n",
    "    i = 0\n",
    "    x = randomArray(size)\n",
    "    m = trainingData.count()\n",
    "    f_tab = np.zeros(MAX_ITER)\n",
    "    while i < MAX_ITER:\n",
    "        print(i, end=' ')\n",
    "        f = trainingData.map(lambda line : (1/m) * logistic_grad_per_example(line,x)).sum()\n",
    "        f_tab[i] = trainingData.map(lambda line : (1/m) * logistic_loss_per_example(line,x)).sum()\n",
    "        x = x - gamma * f    \n",
    "        i += 1\n",
    "    return x, f_tab\n",
    "\n",
    "#MAX_ITER = 20\n",
    "#x, f_tab = grad_algo(0.66,MAX_ITER)\n",
    "#print(\"\\nx =\",x,\"\\nf_tab =\",f_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6__\n",
    "\n",
    "> Plot the functional value versus the iterations.\n",
    "\n",
    "> Investigate if the computations are distributed over different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAX_ITER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-619031b18eef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_ITER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_ITER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_ITER' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff6c0ca198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(MAX_ITER), f_tab, color=\"red\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, MAX_ITER)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logisitic regression\n",
    "\n",
    "In addition to the loss, it is usual to add a regularization term of the form\n",
    "$$ r(x) = \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "The first part promotes sparsity of the iterates while the second part prevents over-fitting. \n",
    "This kind of regularization is often called:\n",
    "- *elastic-net* when $ \\lambda_1$ and $ \\lambda_2$ are non-null\n",
    "- $\\ell_1$ when $\\lambda_2 = 0$\n",
    "- *Tikhonov* when $\\lambda_1 = 0$\n",
    "\n",
    "The full optimization problems now writes\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } g(x) =  \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) +  \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "__Question 7__\n",
    "\n",
    "> Which part of $g$ is smooth, which part is not? Write $g$ as \n",
    "$$ g(x) =  \\frac{1}{m}  \\sum_{i=1}^m s_i(x) + n(x)  $$\n",
    "where the $(s_i)$ are smooth function and $n$ is non smooth.\n",
    "\n",
    "> ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    ">      --> \\lambda_1 \\|x\\|_1 is not differentiable at 0 --> not smooth function\n",
    "\n",
    ">      The reasons for regularization are \n",
    "\n",
    ">       1) to avoid overfitting by not generating high coefficients for predictors that are sparse.  \n",
    "\n",
    ">       2) to stabilize the estimates especially when there's collinearity in the data.  \n",
    "\n",
    "> ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "> Define a function `regularized_logistic_grad_per_example(examples,x)` returning the gradient of the smooth part per example (i.e. $\\nabla s_i(x)$)\n",
    "\n",
    "> Define a function `n_prox(x,gamma)` returning the proximal operator of the non-smooth part (i.e. $\\mathbf{prox}_{\\gamma n}(y)$)\n",
    "\n",
    "we recall that\n",
    "$$ \\mathbf{prox}_{\\gamma n}(y) = \\arg\\min_x\\left\\{ n(x) + \\frac{1}{2\\gamma} \\|x-y\\|_2^2 \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regularized logistic grad for the first example is :\n",
      " [-0.109386701112,0.568547703558,0.559973599285,-0.390406590755,0.28120471103,0.285802646937,-0.282021905114,0.516828785939,-0.933190778825,-0.0744266181621,-0.871214618803,0.0326383040313,-0.360724869734,0.318960165725,0.160975273999,-0.701513316441,-0.647388819868,0.535927968234,0.760465417661,-0.80266992594,0.16253660176,0.136337283387,-0.858859003224,-0.0214422338853,0.653281586372,-0.349488107016,0.253324358183,0.503516564517,0.457120377222,-0.536226878762,0.0848987602049,-0.610676921458,-0.269817304756,-0.804608766213,0.00859294945814] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Part I\n",
    "m = trainingData.count()\n",
    "def regularized_logistic_grad_per_example(examples,x,lambda2):\n",
    "    return logistic_grad_per_example(examples,x) + 2 * lambda2 * x \n",
    "\n",
    "testRegularizedGradientLoss = trainingData.map(lambda line : regularized_logistic_grad_per_example(line,randomArray(size),0.5))\n",
    "print(\"The regularized logistic grad for the first example is :\\n\",testRegularizedGradientLoss.first(),\"\\n\")\n",
    "\n",
    "# Part II\n",
    "def n_prox(x,gamma):\n",
    "    res = np.zeros(size)\n",
    "    for i in range(1,size):\n",
    "        if x[i] > gamma :\n",
    "            res[i] = x[i] - gamma\n",
    "        elif x[i] < -gamma :\n",
    "            res[i] = x[i] + gamma\n",
    "        else :\n",
    "            res[i] = 0\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8__\n",
    "\n",
    "> Compute a proximal gradient algorithm for computing a solution of\n",
    "$$ \\min_x  f(x) + r(x) = \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\lambda_1 \\|x\\|_1 + \\lambda_2 \\|x\\|^2_2 $$\n",
    "\n",
    "\n",
    "Hint: An admissible stepsize can be found by taking $\\gamma = 1/L_{b2}$ with  $ L_b = \\max_i 0.25 \\|a_i\\|_2^2 + 2\\lambda_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma value : 0.5333333333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part I\n",
    "MAX_ITER = 20\n",
    "def compute_gamma2(lambda2):\n",
    "    Lb = 0.25 * trainingData.map(lambda line : pow(np.linalg.norm(line.features,2),2)).max() \\\n",
    "        + lambda2 * 2 \n",
    "    #print(0.5/Lb)\n",
    "    return 0.8/Lb\n",
    "print(\"Gamma value :\",compute_gamma2(0.5),\"\\n\")\n",
    "\n",
    "# Part II\n",
    "def prox_grad_algo(lambda1,lambda2):\n",
    "    i = 0\n",
    "    gamma = compute_gamma2(lambda2)\n",
    "    x = randomArray(size)\n",
    "    g_tab = np.zeros(MAX_ITER)\n",
    "    while i < MAX_ITER:\n",
    "        print(i, end=' ')\n",
    "        g = trainingData.map(lambda line : (1/m) * regularized_logistic_grad_per_example(line,x,lambda2)).sum()\n",
    "        g_tab[i] = trainingData.map(lambda line : (1/m) * logistic_loss_per_example(line,x)).sum() + \\\n",
    "                                    lambda1 * np.linalg.norm(x,1) + lambda2 * pow(np.linalg.norm(x,2),2)\n",
    "        x = n_prox(x - gamma * g,lambda1*gamma) \n",
    "        i += 1\n",
    "    return x, g_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 9__\n",
    "\n",
    "> Examine the behavior and output of your proximal gradient algorithm with different values of $\\lambda_1$, $\\lambda_2$. What do you observe in terms of sparsity of the solution and convergence rate of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def compute_min(l1,l2):\n",
    "    x, g_tab = prox_grad_algo(l1,l2)\n",
    "    return x, g_tab\n",
    "\n",
    "MAX_ITER = 100\n",
    "lamb1 = [0,0,0,0.1]\n",
    "lamb2 = [0,0.1,0.1,0.1]\n",
    "x     = [0,0,0,0]\n",
    "g_tab = [0,0,0,0]\n",
    "for i in range(0,4):\n",
    "    x[i], g_tab[i] = compute_min(lamb1[i],lamb2[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Courbe 1 [ red ]\n",
      "lambda1 = 0 \n",
      "lambda2 = 0 \n",
      "gamma = 0.19999999999999993\n",
      "0.616402983361 \n",
      "\n",
      "\n",
      "\n",
      "Courbe 2 [ green ]\n",
      "lambda1 = 0 \n",
      "lambda2 = 0.1 \n",
      "gamma = 0.14285714285714282\n",
      "0.991944046596 \n",
      "\n",
      "\n",
      "\n",
      "Courbe 3 [ blue ]\n",
      "lambda1 = 0 \n",
      "lambda2 = 0.1 \n",
      "gamma = 0.14285714285714282\n",
      "1.00233097015 \n",
      "\n",
      "\n",
      "\n",
      "Courbe 4 [ black ]\n",
      "lambda1 = 0.1 \n",
      "lambda2 = 0.1 \n",
      "gamma = 0.14285714285714282\n",
      "1.15163595823 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAF5CAYAAADQ2iM1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51047ac978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_res(x,g_tab,col):\n",
    "    plt.plot(range(MAX_ITER-5), g_tab[5:], col, linewidth=1.0, linestyle=\"-\")\n",
    "\n",
    "colors = [\"red\", \"green\", \"blue\",\"black\"]\n",
    "plt.figure()\n",
    "for i in range(0,4):\n",
    "    print(\"\\nCourbe\", i+1, \"[\",colors[i],\"]\\nlambda1 =\", lamb1[i], \"\\nlambda2 =\",\\\n",
    "        lamb2[i], \"\\ngamma =\", compute_gamma2(lamb2[i]))\n",
    "    plot_res(x[i],g_tab[i],colors[i])\n",
    "    print(g_tab[i][MAX_ITER-1],\"\\n\\n\")\n",
    "plt.xlim(0, MAX_ITER)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Question 10__\n",
    "\n",
    "> Write a function that evaluates the accuracy of the classification on the training dataset.\n",
    "\n",
    "> Investigate how this accuracy change when playing with the regularization terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur pour x[ 0 ] = 0.3373134328358209\n",
      "erreur pour x[ 1 ] = 0.382089552238806\n",
      "erreur pour x[ 2 ] = 0.3283582089552239\n",
      "erreur pour x[ 3 ] = 0.34328358208955223\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "training_size = trainingData.count()\n",
    "def diff_tableau(a,b):\n",
    "    s = a\n",
    "    for i in range(0,training_size):\n",
    "        s[i] = a[i] - b[i]\n",
    "    return s\n",
    "\n",
    "#Labelisation\n",
    "def p(x,example):\n",
    "      #  label = 0\n",
    "    p = 1/(1+exp(-1 * np.dot(example.features,x)))\n",
    "    if p > 0.5:\n",
    "        label = 1.0\n",
    "    else:\n",
    "        label = -1.0\n",
    "    return label\n",
    "\n",
    "# test\n",
    "train = trainingData\n",
    "def test_train(train,x):\n",
    "    return train.map(lambda line : p(x,line))\n",
    "\n",
    "diff = [0,0,0,0]\n",
    "true_label = train.map(lambda line: line.label)\n",
    "#print(\"true_label :\\n\",true_label.collect())\n",
    "for i in range(0,4):\n",
    "    test = test_train(train,x[i])\n",
    "    diff[i] = diff_tableau(test.collect(),true_label.collect())\n",
    "    #print(\"test :\",i,\"\\n\",diff_tableau(test.collect(),true_label.collect()),\"\\n x = \\n\",x[i],\"\\n\")\n",
    "\n",
    "# Part 2\n",
    "def pourcentage_erreur(diff):\n",
    "    return sum([abs(_i) for _i in diff]) /(2*training_size)\n",
    "\n",
    "erreur = [0,0,0,0]\n",
    "for i in range(0,4):\n",
    "    erreur[i] = pourcentage_erreur(diff[i])\n",
    "    print(\"erreur pour x[\",i,\"] =\",erreur[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerations\n",
    "\n",
    "A popular acceleration method to improve the convergence rate of proximal gradient algorithm is the addition of inertia. That is, contructing the next gradient input by a combination of the last two outputs.\n",
    "\n",
    "\n",
    "In particular, Nesterov's acceleration is the most popular form of inertia. It writes\n",
    "$$ \\left\\{ \\begin{array}{l}   y_{k+1} = \\mathbf{prox\\_grad}(x_k) \\\\ x_{k+1} = y_{k+1} + \\alpha_{k+1} (y_{k+1} - y_k)  \\end{array} \\right. $$ \n",
    "with\n",
    "* $\\mathbf{prox\\_grad}$ the proximal gradient operation\n",
    "* $(\\alpha_{k})$ the inertial sequence defined as $\\alpha_k = \\frac{t_k-1}{t_{k+1}}$ and $t_0 = 0$ and $t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$\n",
    "\n",
    "__Question 11__\n",
    "\n",
    "> Implement a fast proximal gradient with this kind of inertia (This algorithm is often nicknamed FISTA).\n",
    "\n",
    "> Compare the convergence speed with the vanilla proximal gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FISTA_prox_grad_algo(lambda1,lambda2):\n",
    "    \n",
    "    t_kPlus1 = 1\n",
    "    \n",
    "    y_k = randomArray(size)\n",
    "    y_kPlus1 = randomArray(size)\n",
    "        \n",
    "    x = randomArray(size)\n",
    "\n",
    "    i = 0\n",
    "    h_tab = np.zeros(MAX_ITER)\n",
    "    gamma = compute_gamma2(lambda2)\n",
    "    \n",
    "    while i < MAX_ITER:\n",
    "        print(i, end =' ')\n",
    "        y_kPlus1 = trainingData.map(lambda line : (1/m) * regularized_logistic_grad_per_example(line,x,lambda2)).sum()                            \n",
    "        h_tab[i] = trainingData.map(lambda line : (1/m) * logistic_loss_per_example(line,x)).sum()\n",
    "        t_kPlus2 = (1 + sqrt(1 + 4 * t_kPlus1 * t_kPlus1)) / 2\n",
    "        alpha_kPlus1 = (t_kPlus1 - 1) / t_kPlus2\n",
    "        x = y_kPlus1 + alpha_kPlus1 * (y_kPlus1 - y_k)\n",
    "        \n",
    "        i += 1\n",
    "        y_k = y_kPlus1\n",
    "        t_kPlus1 = t_kPlus2\n",
    "    return x, h_tab\n",
    "MAX_ITER = 1\n",
    "x, h_tab = FISTA_prox_grad_algo(0,0)\n",
    "print(\"\\nx =\",x,\"\\nh_tab =\",h_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe30d2d5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"red\", \"green\", \"blue\",\"black\"]\n",
    "plt.figure()\n",
    "#for i in range(0,4):\n",
    "#    print(\"\\nCourbe\", i+1, \"[\",colors[i],\"]\\nlambda1 =\", lamb1[i], \"\\nlambda2 =\",\\\n",
    "#        lamb2[i], \"\\ngamma =\", compute_gamma2(lamb2[i]))\n",
    "#    plot_res(x[i],g_tab[i],colors[i])\n",
    "plt.plot(range(MAX_ITER), h_tab, color=\"yellow\", linewidth=1.0, linestyle=\"-\")\n",
    "plt.xlim(0, MAX_ITER)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Functional value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental methods\n",
    "\n",
    "When dimension increases, incremental algorithms are often priviledged. \n",
    "\n",
    "A possible incremental algorithm for a problem such as regularized logistic regression is MISO (see *J Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning. SIAM Journal on Optimization,2015 and ICML 2014.*):\n",
    "\n",
    "* Draw randomly a sample $n$\n",
    "* Compute $x^n_{k+1} = \\mathbf{prox}_{\\gamma g} (\\bar{x}_k) - \\gamma \\nabla f_n(\\mathbf{prox}_{\\gamma g} (\\bar{x}_k) )$\n",
    "* For all $i\\neq n$, $x^i_{k+1}=x^i_k$ \n",
    "* Compute new $\\bar{x}_{k+1} = \\frac{1}{m} \\sum_{j=1}^m x^j_{k+1}$\n",
    " \n",
    "\n",
    "__Question 12__\n",
    "\n",
    "> Implement this incremental algorithm and compare with the previous algorithms in terms of convergence time and functional value versus number of passes over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
